#!/usr/bin/env python

import os
import sys
import time
import fnmatch
import sqlite3
import re
import threading
from argparse import ArgumentParser

parser = ArgumentParser(description = 'Update replica information.')
parser.add_argument('--config', '-c', metavar = 'CONFIG', dest = 'config', required = True, help = 'Configuration JSON.')
parser.add_argument('--mode', '-m', metavar = 'MODE', dest = 'mode', help = 'Automatic update mode. Values are: ReplicaDelta, ReplicaFull. When set, --site and --dataset are ignored.')
parser.add_argument('--site', '-s', metavar = 'SITE', dest = 'sites', nargs = '+', help = 'Do a full update of the site (wildcard allowed). Prepend ! to veto match.')
parser.add_argument('--dataset', '-d', metavar = 'DATASET', dest = 'datasets', nargs = '+', help = 'Do a full update of the dataset (wildcard allowed).')
parser.add_argument('--delta-since', '-t', metavar = 'TIME', dest = 'delta_since', type = int, help = 'With --mode=ReplicaDelta, override the update_since timestamp.')

args = parser.parse_args()
sys.argv = []

## Load the configuration
from dynamo.core.executable import make_standard_logger, authorized, inventory
from dynamo.dataformat import Dataset, Block, File, Group, DatasetReplica, BlockReplica, ObjectError
from dynamo.dataformat.configuration import Configuration
from dynamo.dataformat.history import DeletedReplica, CopiedReplica, HistoryRecord
from dynamo.source.impl.phedexgroupinfo import PhEDExGroupInfoSource
from dynamo.source.impl.phedexsiteinfo import PhEDExSiteInfoSource
from dynamo.source.impl.phedexdatasetinfo import PhEDExDatasetInfoSource
from dynamo.source.impl.phedexreplicainfo import PhEDExReplicaInfoSource
from dynamo.operation.impl.phedexcopy import PhEDExCopyInterface
from dynamo.operation.impl.phedexdeletion import PhEDExDeletionInterface
from dynamo.operation.history import DeletionHistoryDatabase, CopyHistoryDatabase
from dynamo.fileop.rlfsm import RLFSM
from dynamo.utils.interface.mysql import MySQL
from dynamo.utils.parallel import Map

config = Configuration(args.config)

## Set up logging (write to stdout & stderr)

LOG = make_standard_logger(config.get('log_level', 'info'))

## Option compatibilities
if args.mode not in [None, 'ReplicaDelta', 'ReplicaFull']:
    LOG.error('Unknown automatic update mode %s.', args.mode)
    sys.exit(1)

if args.mode is None and args.sites is None and args.datasets is None:
    LOG.error('At least one of --site --dataset is needed for non-automatic update.')
    sys.exit(1)

if args.mode and not os.path.exists(config.updater_state_file):
    LOG.error('State file %s does not exist. Run generate_dataset_list_cms first.', config.replica_state_file)
    sys.exit(1)

LOG.info('Starting inventory update.')

## Load and initialize sources

if 'sites' not in config:
    config.sites = Configuration()
if 'datasets' not in config:
    config.datasets = Configuration()
if 'replicas' not in config:
    config.replicas = Configuration()

if 'include_sites' in config:
    config.sites.include = config.include_sites
    config.replicas.include_sites = config.include_sites
if 'exclude_sites' in config:
    config.sites.exclude = config.exclude_sites
    config.replicas.exclude_sites = config.exclude_sites
if 'include_datasets' in config:
    config.datasets.include = config.include_datasets
    config.replicas.include_datasets = config.include_datasets
if 'exclude_datasets' in config:
    config.datasets.exclude = config.exclude_datasets
    config.replicas.exclude_datasets = config.exclude_datasets

group_source = PhEDExGroupInfoSource(config.get('groups', None))
site_source = PhEDExSiteInfoSource(config.sites)
dataset_source = PhEDExDatasetInfoSource(config.datasets)
replica_source = PhEDExReplicaInfoSource(config.replicas)

## Tools for file-level operations

rlfsm = RLFSM()
phedex_copy = PhEDExCopyInterface()
phedex_deletion = PhEDExDeletionInterface()
copy_history = CopyHistoryDatabase()
deletion_history = DeletionHistoryDatabase()

if not authorized:
    rlfsm.set_read_only()
    phedex_copy.set_read_only()
    phedex_deletion.set_read_only()
    copy_history.set_read_only()
    deletion_history.set_read_only()

## Reservations DB (keeping track of in-house operations we did)

reservations_db = MySQL(config.reservations_db_params)

# Pre-fetch the list of reserved transfers before we deal with deletions
transfer_reservations = {} # {(dataset, site): (set(blocks), group, reservation id)}

for rid, item_name, site_name, group_name in reservations_db.query('SELECT `id`, `item`, `site`, `group` FROM `phedex_transfer_reservations`'):
    try:
        dataset_name, block_name = Block.from_full_name(item_name)
    except ObjectError:
        dataset_name = item_name
        block_name = None

    try:
        dataset = inventory.datasets[dataset_name]
        site = inventory.sites[site_name]
        group = inventory.groups[group_name]
    except KeyError:
        # shouldn't happen
        if authorized:
            reservations_db.query('DELETE FROM `phedex_transfer_reservations` WHERE `id` = %s', rid)
        continue

    if block_name is None:
        blocks = set(dataset.blocks)
    else:
        block = dataset.find_block(block_name)
        if block is None:
            # shouldn't happen
            if authorized:
                reservations_db.query('DELETE FROM `phedex_deletion_reservations` WHERE `id` = %s', rid)
            continue

        blocks = set([block])

    transfer_reservations[(dataset, site)] = (blocks, group, rid)

## Start the update
# 1. Refresh groups
# 2. Get the list of block replicas and dataset names to update
# 3. Update the datasets
# 4. Loop over new and changed block replicas, add them to inventory
# 5. Pick up deleted block replicas
# 6. Loop over deleted replicas
# 7. Save the execution state

parallelizer = Map()
parallelizer.timeout = 7200

## 1. Refresh groups

LOG.info('Updating list of groups.')
for group in group_source.get_group_list():
    LOG.debug('Updating %s', str(group))
    inventory.update(group)

## 2. Get the list of block replicas and dataset names to update

if args.mode == 'ReplicaDelta':
    ## Global delta-update of replicas
    ## Get the last update timestamp

    if args.delta_since:
        last_update = args.delta_since

    else:
        state_db = sqlite3.connect(config.updater_state_file)
        cursor = state_db.cursor()
    
        sql = 'SELECT MAX(`timestamp`) FROM `replica_delta_updates`'
        result = cursor.execute(sql)
        last_update = next(result)[0]
    
        if last_update is None:
            LOG.error('Last update timestamp is not set. Run a full update of all sites and create a timestamp.')
            sys.exit(1)
    
        state_db.close()
    
        # Allow 30-minute safety margin to fully collect all updates
        # Yes we really need that much (have seen as far as 20 minute delay in blockreplicas)
        last_update -= 1800

    # The timestamp for this update
    update_start = time.time()
    
    ## Fetch the full list of block replicas that were updated since updated_since.
    ## New datasets and blocks will be caught in the process.
    
    timestamp_str = time.strftime('%Y-%m-%d %H:%M:%S %Z', time.localtime(last_update))
    LOG.info('Fetching the list of block replicas updated since %s.', timestamp_str)

    deleted_replicas = []

    # Fetch deleted replicas in a separate thread
    def get_deleted_replicas():
        replicas = replica_source.get_deleted_replicas(last_update)
        deleted_replicas.extend(replicas)

    thr = threading.Thread(target = get_deleted_replicas)
    thr.start()

    updated_replicas = replica_source.get_updated_replicas(last_update)

    thr.join()

    # Sometimes when a subscription is made but is removed before any transfer happens, PhEDEx "deletions" call
    # returns nothing and the empty subscription stays in the inventory, inflating the projected volume at the site.
    # We need to check all empty replicas with PhEDEx.
    lock = threading.Lock()
    def check_empty_replica(replica):
        if not replica_source.replica_exists_at_site(replica.site, replica.dataset):
            with lock:
                deleted_replicas.extend(replica.block_replicas)

    empty_replicas = []
    for site in inventory.sites.itervalues():
        for replica in site.dataset_replicas():
            if replica.size(physical = True) == 0:
                empty_replicas.append(replica)

    parallelizer.execute(check_empty_replica, empty_replicas)
        
    # Names of all the datasets to update
    dataset_names = set(br.block.dataset.name for br in updated_replicas)

elif args.mode == 'ReplicaFull':
    ## Round-robin update of a site-tier combination
    ## Get the combination to run on

    state_db = sqlite3.connect(config.updater_state_file)
    cursor = state_db.cursor()

    updated_replicas = []
    site_dataset_combos = []

    sql = 'SELECT `site`, `tier` FROM `replica_full_updates` ORDER BY `id` ASC'
    result = cursor.execute(sql)

    while len(updated_replicas) < 3000:
        try:
            site, tier = next(result)
        except StopIteration:
            LOG.error('Round robin state table is empty. Run generate_dataset_list_cms first.')
            sys.exit(0)

        dataset = '/*/*/' + tier

        updated_replicas.extend(replica_source.get_replicas(site = site, dataset = dataset))
        site_dataset_combos.append((str(site), str(dataset))) # sqlite3 gives us unicode

    LOG.info('Performing full inventory update for combinations %s', site_dataset_combos)

    state_db.close()

    dataset_names = set(br.block.dataset.name for br in updated_replicas)

else:
    if args.sites:
        site_dataset_combos = []
        site_list = None
        for site_name in args.sites:
            if site_name.startswith('!'):
                # remove matching site from list filled so far
                site_pat = re.compile(fnmatch.translate(site_name[1:]))

                ic = 0
                while ic != len(site_dataset_combos):
                    if site_pat.match(site_dataset_combos[ic][0]):
                        site_dataset_combos.pop(ic)
                    else:
                        ic += 1

                continue

            if '*' in site_name:
                # expand site name already so that we don't download a humongous replica list
                site_pat = re.compile(fnmatch.translate(site_name))

                if site_list is None:
                    site_list = site_source.get_site_list()

                for site in site_list:
                    if site_pat.match(site.name):
                        site_dataset_combos.append((site.name, None))

            else:
                site_dataset_combos.append((site_name, None))
                    
    else:
        site_dataset_combos = [(None, None)]

    if args.datasets:
        site_dataset_combos = [(c[0], d) for d in args.datasets for c in site_dataset_combos]

    updated_replicas = []
    for site_name, dataset_name in site_dataset_combos:
        updated_replicas.extend(replica_source.get_replicas(site = site_name, dataset = dataset_name))

    dataset_names = set(br.block.dataset.name for br in updated_replicas)

# 3. Update the datasets

# 3.1. Query the dataset source (parallelize)

LOG.info('Updating dataset information.')

def get_dataset(name):
    LOG.info('Updating information for dataset %s', name)
    
    dataset_tmp = dataset_source.get_dataset(name, with_files = True)

    if dataset_tmp is None:
        LOG.error('Unknown dataset %s.', name)
        
    return dataset_tmp

dataset_tmps = parallelizer.execute(get_dataset, dataset_names, async = True)

def create_file(file_tmp, block):
    LOG.debug('Creating new file %s', file_tmp.lfn)
    lfile = File(file_tmp.lfn, block)
    lfile.copy(file_tmp)
    block.add_file(lfile)
    inventory.register_update(lfile)

def create_block(block_tmp, dataset):
    LOG.debug('Creating new block %s', block_tmp.full_name())
    block = Block(block_tmp.name, dataset)
    block.copy(block_tmp)
    dataset.blocks.add(block)
    inventory.register_update(block)

    # add files
    for file_tmp in block_tmp.files:
        create_file(file_tmp, block)

def create_dataset(dataset_tmp):
    LOG.debug('Creating new dataset %s', dataset_tmp.name)
    dataset = Dataset(dataset_tmp.name)
    dataset.copy(dataset_tmp)
    inventory.datasets.add(dataset)
    inventory.register_update(dataset)

    # add blocks
    for block_tmp in dataset_tmp.blocks:
        create_block(block_tmp, dataset)

watermark = 0
idat = 0

for dataset_tmp in dataset_tmps:
    if float(idat) / len(dataset_names) * 100. >= watermark:
        LOG.info('%d%% done..', watermark)
        watermark += 5

    idat += 1

    if dataset_tmp is None:
        continue

    # 3.2. Find the dataset or create new

    try:
        dataset = inventory.datasets[dataset_tmp.name]
    except KeyError:
        create_dataset(dataset_tmp)
        continue

    if dataset != dataset_tmp:
        LOG.debug('Updating dataset %s', dataset.name)
        dataset.copy(dataset_tmp)
        inventory.register_update(dataset)

    # 3.3. Update blocks

    existing_blocks = dict((b.name, b) for b in dataset.blocks)
    block_names_in_source = set()

    for block_tmp in dataset_tmp.blocks:
        block_names_in_source.add(block_tmp.name)

        try:
            block = existing_blocks[block_tmp.name]
        except KeyError:
            create_block(block_tmp, dataset)
            continue

        if block == block_tmp:
            # If num_files and size are identical, there is very little chance that the actual file list is different.
            # We skip the file update.
            continue
        else:
            LOG.debug('Updating block %s', block.full_name())
            block.copy(block_tmp)
            inventory.register_update(block)

        # 3.4. Update files

        existing_files = dict(((f.lfn, f) for f in block.files))
        file_names_in_source = set()

        for file_tmp in block_tmp.files:
            lfn = file_tmp.lfn
            file_names_in_source.add(lfn)

            try:
                lfile = existing_files[lfn]
            except KeyError:
                create_file(file_tmp, block)
                continue

            if lfile != file_tmp:
                LOG.debug('Updating file %s', lfile.lfn)
                lfile.copy(file_tmp)
                inventory.register_update(lfile)

        # 3.5. Delete excess files

        for lfn in (set(existing_files.iterkeys()) - file_names_in_source):
            LOG.debug('Deleting file %s', lfn)
            inventory.delete(existing_files[lfn])

    # 3.6. Delete excess blocks

    for block_name in (set(existing_blocks.iterkeys()) - block_names_in_source):
        LOG.debug('Deleting block %s', block_name)
        inventory.delete(existing_blocks[block_name])

LOG.info('100% done.')

## 4. Loop over new and changed block replicas, add them to inventory

num_replicas = len(updated_replicas)
LOG.info('Got %d block replicas to update.', num_replicas)

# Save the embedded versions - we cannot query for "replicas deleted since X", so instead compare
# what is already in the database to what we get from PhEDEx.
embedded_updated_replicas = set()

group = None
site = None
dataset = None
dataset_replica = None

for replica in updated_replicas:
    replica_str = str(replica)

    LOG.debug('Checking %s', replica_str)

    # 4.1. Pick up replicas of known groups only

    if group is None or group.name != replica.group.name:
        try:
            group = inventory.groups[replica.group.name]
        except KeyError:
            LOG.debug('%s is owned by %s, which is not a tracked group.', replica_str, replica.group.name)
            continue

    # 4.2. Pick up replicas at known sites only

    if site is None or site.name != replica.site.name:
        try:
            site = inventory.sites[replica.site.name]
        except KeyError:
            LOG.debug('%s is at %s, which is not a tracked site.', replica_str, replica.site.name)
            continue

        dataset_replica = None

    # 4.3. Update the dataset info

    if dataset is None or dataset.name != replica.block.dataset.name:
        dataset_name = replica.block.dataset.name
        if not dataset_name.startswith('/') or dataset_name.count('/') != 3:
            continue

        # valid datasets should exist in the repository now
        dataset = inventory.datasets[dataset_name]
        dataset_replica = None

    # 4.4. Find the dataset replica

    if dataset_replica is None:
        dataset_replica = site.find_dataset_replica(dataset)

        if (dataset, site) in transfer_reservations:
            replica_reservation = transfer_reservations[(dataset, site)]
        else:
            replica_reservation = None

    if dataset_replica is None:
        # If not found, create a new replica and inject
        LOG.info('Creating new replica of %s at %s', dataset.name, site.name)
        dataset_replica = DatasetReplica(dataset, site)

        dataset.replicas.add(dataset_replica)
        site.add_dataset_replica(dataset_replica, add_block_replicas = False)

        inventory.register_update(dataset_replica)

    # 4.5. Find the block of the dataset

    block = dataset.find_block(replica.block.name)

    if block is None:
        LOG.error('Unknown block %s.', replica.block.full_name())
        continue

    if replica_reservation is not None:
        # Something external already subscribed this block. Remove it from the reservation
        try:
            replica_reservation[0].remove(block)
        except KeyError:
            continue

    # 4.6. Update the block replica

    block_replica = block.find_replica(site)

    if block_replica is None:
        LOG.debug('Creating new replica of %s at %s', block.full_name(), site.name)
        block_replica = BlockReplica(block, site, group)
        block_replica.copy(replica)
        # reset the group
        block_replica.group = group

        dataset_replica.block_replicas.add(block_replica)
        block.replicas.add(block_replica)
        site.add_block_replica(block_replica)

        inventory.register_update(block_replica)

    elif block_replica != replica:
        LOG.debug('Updating %s', replica_str)
        block_replica.copy(replica)
        inventory.register_update(block_replica)
        
    if args.mode != 'ReplicaDelta':
        embedded_updated_replicas.add(block_replica)

## 5. Pick up deleted block replicas

# deleted_replicas in ReplicaDelta mode is fetched together with updated_replicas
if args.mode != 'ReplicaDelta':
    # direct or ReplicaFull. Lucky we have args.site and args.dataset defined in ReplicaFull.
    # Replicas in inventory but not in updated_replicas are deleted
    deleted_replicas = []

    # find block replicas that exist in the database but not in embedded_updated_replicas
    # for each site & dataset combination
    for site_name, dataset_name in site_dataset_combos:
        if site_name:
            site_pat = re.compile(fnmatch.translate(site_name))
        else:
            site_pat = None

        if dataset_name:
            dataset_pat = re.compile(fnmatch.translate(dataset_name))
        else:
            dataset_pat = None

        for site in inventory.sites.itervalues():
            if site_pat and not site_pat.match(site.name):
                continue
    
            for dataset_replica in site.dataset_replicas():
                if dataset_pat and not dataset_pat.match(dataset_replica.dataset.name):
                    continue
    
                for block_replica in dataset_replica.block_replicas:
                    if block_replica not in embedded_updated_replicas:
                        deleted_replicas.append(block_replica)

## 6. Loop over deleted replicas

for replica in deleted_replicas:
    replica_str = str(replica)

    LOG.debug('Deleting %s', replica_str)

    # 6.1. Pick up replicas of known groups only

    if replica.group.name not in inventory.groups:
        LOG.debug('%s is owned by %s, which is not a tracked group.', replica_str, replica.group.name)
        continue

    # 6.2. Pick up replicas at known sites only

    try:
        site = inventory.sites[replica.site.name]
    except KeyError:
        LOG.debug('%s is at %s, which is not a tracked site.', replica_str, replica.site.name)
        continue

    # 6.3. Find the dataset in the inventory

    try:
        dataset = inventory.datasets[replica.block.dataset.name]
    except KeyError:
        # If not found, create a new dataset and inject
        LOG.debug('Unknown dataset %s.', replica.block.dataset.name)
        continue

    # 6.4. Find the block of the dataset

    block_full_name = replica.block.full_name()
    block = dataset.find_block(replica.block.name)

    if block is None:
        # If not found, create a new block and inject
        LOG.debug('Unknown block %s.', block_full_name)
        continue

    # 6.5. Find the dataset replica

    dataset_replica = site.find_dataset_replica(dataset)
    if dataset_replica is None:
        LOG.debug('No replica of %s at %s.', dataset.name, site.name)
        continue

    # 6.6. Delete the block replica

    # blockreplica.unlink_from() raises a KeyError or ObjectError if
    # any of the group, site, dataset, ... is not found
    # Containing dataset replica will be deleted within blockreplica.unlink()
    # if it becomes empty
    try:
        inventory.delete(replica)
    except (KeyError, ObjectError):
        LOG.debug('Replica not found.')
        pass

# 7. Save the execution state

if authorized and args.mode is not None and os.path.exists(config.updater_state_file):
    # Regardless of update mode, clear the updated datasets from the table
    state_db = sqlite3.connect(config.updater_state_file)
    cursor = state_db.cursor()

    # Additionally for replica updates
    if args.mode == 'ReplicaDelta':
        sql = 'INSERT INTO `replica_delta_updates` VALUES (?, ?, ?)'
        cursor.execute(sql, (update_start, len(updated_replicas), len(deleted_replicas)))

    elif args.mode == 'ReplicaFull':
        for site_name, dataset_name in site_dataset_combos:
            tier = dataset_name[dataset_name.rfind('/') + 1:]
            cursor.execute('DELETE FROM `replica_full_updates` WHERE `site` = ? AND `tier` = ?', (site_name, tier))

    state_db.commit()
    state_db.close()

# 8. Deal with Dynamo-run transfers and deletions

LOG.info('Collecting in-house transfers and deletions.')

subscriptions = rlfsm.get_subscriptions(inventory, status = ['done'])

completed_subscriptions = {}
transferred_files = {}
completed_desubscriptions = {}
deleted_files = {}

for sub in subscriptions:
    key = (sub.file.block, sub.destination)

    if type(sub) is RLFSM.Subscription:
        try:
            completed_subscriptions[key].append(sub.id)
            transferred_files[key].append(sub.file.id)
        except KeyError:
            completed_subscriptions[key] = [sub.id]
            transferred_files[key] = [sub.file.id]
    else:
        try:
            completed_desubscriptions[key].append(sub.id)
            deleted_files[key].append(sub.file.id)
        except KeyError:
            completed_desubscriptions[key] = [sub.id]
            deleted_files[key] = [sub.file.id]

for key, file_ids in transferred_files.iteritems():
    block, site = key
    replica = block.find_replica(site)

    if replica is None:
        # replica doesn't exist yet
        if set(f.id for f in block.files) != set(file_ids):
            # and it's not complete yet
            completed_subscriptions.pop(key)

    elif replica.file_ids is None:
        # is already full
        pass

    else:
        file_ids.extend(replica.file_ids)
        if set(f.id for f in block.files) != set(file_ids):
            completed_subscriptions.pop(key)

for key, file_ids in deleted_files.iteritems():
    block, site = key
    replica = block.find_replica(site)

    if replica is None:
        # didn't even exist to begin with
        pass
    elif replica.file_ids is None:
        if set(f.id for f in block.files) != set(file_ids):
            completed_desubscriptions.pop(key)
    else:
        if set(replica.file_ids) > set(file_ids):
            completed_desubscriptions.pop(key)

# Completed_subscriptions and completed_desubscriptions now contain (block, site) => [subscription ids] of (de)subscriptions
# that may be ready to be reported to PhEDEx

done_subscription_ids = []

# First deal with copies

done_transfer_reservation_ids = []
copy_replica_list = {}

for (dataset, site), (blocks, group, rid) in transfer_reservations.iteritems():
    dataset_done_ids = []
    for block in blocks:
        try:
            dataset_done_ids.extend(completed_subscriptions[(block, site)])
        except KeyError:
            break

    else:
        # All blocks completed
        done_subscription_ids.extend(dataset_done_ids)
        done_transfer_reservation_ids.append(rid)

        # blockreplicas below are created with size = 0 because PhEDEx must ultimately confirm the replica and report back

        if blocks == dataset.blocks:
            # dataset replica reserved
            replica = DatasetReplica(dataset, site, growing = True, group = group)
            copy_replica_list[(dataset, site)] = replica

        else:
            try:
                replica = copy_replica_list[(dataset, site)]
            except KeyError:
                replica = copy_replica_list[(dataset, site)] = DatasetReplica(dataset, site, growing = False)
            else:
                if replica.growing:
                    continue

        for block in blocks:
            replica.block_replicas.add(BlockReplica(block, site, group, size = 0))
            
# Then deal with deletions

done_deletion_reservation_ids = []
deletion_replica_list = {}

for rid, item_name, site_name in reservations_db.query('SELECT `id`, `item`, `site` FROM `phedex_deletion_reservations`'):
    try:
        dataset_name, block_name = Block.from_full_name(item_name)
    except ObjectError:
        dataset_name = item_name
        block_name = None

    try:
        dataset = inventory.datasets[dataset_name]
        site = inventory.sites[site_name]
    except KeyError:
        # shouldn't happen
        if authorized:
            reservations_db.query('DELETE FROM `phedex_deletion_reservations` WHERE `id` = %s', rid)
        continue

    replica = site.find_dataset_replica(dataset)
    if replica is None:
        # already gone
        if authorized:
            reservations_db.query('DELETE FROM `phedex_deletion_reservations` WHERE `id` = %s', rid)
        continue

    if block_name is None:
        dataset_done_ids = []
        for block in dataset.blocks:
            try:
                dataset_done_ids.extend(completed_desubscriptions[(block, site)])
            except KeyError:
                break

        else:
            # All blocks completed
            done_subscription_ids.extend(dataset_done_ids)
            done_deletion_reservation_ids.append(rid)

            deletion_replica_list[replica] = None

    else:
        block = dataset.find_block(block_name)
        if block is None:
            # shouldn't happen
            if authorized:
                reservations_db.query('DELETE FROM `phedex_deletion_reservations` WHERE `id` = %s', rid)
            continue

        try:
            done_subscription_ids.extend(completed_subscriptions[(block, site)])
        except KeyError:
            continue

        done_deletion_reservation_ids.append(rid)

        block_replica = block.find_replica(site)

        if replica not in deletion_replica_list:
            deletion_replica_list[replica] = [block_replica]
        elif deletion_replica_list[replica] is not None:
            deletion_replica_list[replica].append(block_replica)

# Execute on PhEDEx

by_site = {}
for replica in copy_replica_list.itervalues():
    try:
        by_site[replica.site].append(replica)
    except KeyError:
        by_site[replica.site] = [replica]

for site, replicas in by_site.iteritems():
    history_record = copy_history.make_entry(site.name)

    scheduled_replicas = phedex_copy.schedule_copies(replicas, history_record.operation_id)

    for replica in scheduled_replicas:
        history_record.replicas.append(CopiedReplica(replica.dataset.name, replica.size(physical = False), HistoryRecord.ST_ENROUTE))

        inventory.update(replica)
        for block_replica in replica.block_replicas:
            inventory.update(block_replica)

    copy_history.update_entry(history_record)

by_site = {}
for replica, block_replicas in deletion_replica_list.iteritems():
    try:
        by_site[replica.site].append((replica, block_replicas))
    except KeyError:
        by_site[replica.site] = [(replica, block_replicas)]

for site, replica_list in by_site.iteritems():
    history_record = deletion_history.make_entry(site.name)

    scheduled_replicas = phedex_deletion.schedule_copies(replica_list, history_record.operation_id)

    for replica, block_replicas in scheduled_replicas:
        deleted_size = 0

        if block_replicas is None:
            replica.growing = False
            replica.group = Group.null_group
            # replica is a clone -> use inventory.update instead of inventory.register_update
            inventory.update(replica)

            original_replica = replica.site.find_dataset_replica(replica.dataset)
            for block_replica in original_replica.block_replicas:
                block_replica.group = Group.null_group
                inventory.register_update(block_replica)

            deleted_size += original_replica.size()

        else:
            for block_replica in block_replicas:
                block_replica.group = Group.null_group
                inventory.update(block_replica)

                deleted_size += block_replica.size

        history_record.replicas.append(DeletedReplica(replica.dataset.name, deleted_size))

    deletion_history.update_entry(history_record)

# Remove completed reservations
if authorized:
    reservations_db.delete_many('phedex_transfer_reservations', 'id', done_transfer_reservation_ids)
    reservations_db.delete_many('phedex_deletion_reservations', 'id', done_deletion_reservation_ids)

# Remove subscription entries with status 'done'
# This is dangerous though - if inventory update fails on the server side for some reason,
# we might not see the inconsistency for a while.
rlfsm.close_subscriptions(done_subscription_ids)

LOG.info('Inventory update completed.')
