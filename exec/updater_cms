#!/usr/bin/env python

import os
import sys
import time
import fnmatch
import sqlite3
import re
import threading
from argparse import ArgumentParser

parser = ArgumentParser(description = 'Update replica information.')
parser.add_argument('--config', '-c', metavar = 'CONFIG', dest = 'config', required = True, help = 'Configuration JSON.')
parser.add_argument('--mode', '-m', metavar = 'MODE', dest = 'mode', help = 'Automatic update mode. Values are: ReplicaDelta, ReplicaFull. When set, --site and --dataset are ignored.')
parser.add_argument('--site', '-s', metavar = 'SITE', dest = 'sites', nargs = '+', help = 'Do a full update of the site (wildcard allowed). Prepend ! to veto match.')
parser.add_argument('--dataset', '-d', metavar = 'DATASET', dest = 'datasets', nargs = '+', help = 'Do a full update of the dataset (wildcard allowed).')
parser.add_argument('--dataset-only', '-D', action = 'store_true', dest = 'dataset_only', help = 'Update dataset information only (no replicas).')
parser.add_argument('--delta-since', '-t', metavar = 'TIME', dest = 'delta_since', type = int, help = 'With --mode=ReplicaDelta, override the update_since timestamp.')

args = parser.parse_args()
sys.argv = []

## Load the configuration
from dynamo.dataformat.configuration import Configuration

config = Configuration(args.config)

## Set up logging (write to stdout & stderr)
from dynamo.core.executable import make_standard_logger, read_only, inventory

LOG = make_standard_logger(config.log_level)

## Option compatibilities
if args.mode not in [None, 'ReplicaDelta', 'ReplicaFull']:
    LOG.error('Unknown automatic update mode %s.', args.mode)
    sys.exit(1)

if args.mode is None and args.sites is None and args.datasets is None:
    LOG.error('At least one of --site --dataset is needed for non-automatic update.')
    sys.exit(1)

if args.mode and not os.path.exists(config.updater_state_file):
    LOG.error('State file %s does not exist. Run generate_dataset_list_cms first.', config.replica_state_file)
    sys.exit(1)

if args.dataset_only and not args.datasets:
    LOG.error('Option --dataset-only requires --dataset.')
    sys.exit(1)

LOG.info('Starting inventory update.')

## Load and initialize sources
import dynamo.source.impl as sources

if 'include_sites' in config:
    config.sites.config.include = config.include_sites
    config.replicas.config.include_sites = config.include_sites
if 'exclude_sites' in config:
    config.sites.config.exclude = config.exclude_sites
    config.replicas.config.exclude_sites = config.exclude_sites
if 'include_datasets' in config:
    config.datasets.config.include = config.include_datasets
    config.replicas.config.include_datasets = config.include_datasets
if 'exclude_datasets' in config:
    config.datasets.config.exclude = config.exclude_datasets
    config.replicas.config.exclude_datasets = config.exclude_datasets

group_source = sources.PhEDExGroupInfoSource(config.groups.config)
site_source = sources.PhEDExSiteInfoSource(config.sites.config)
dataset_source = sources.PhEDExDatasetInfoSource(config.datasets.config)
replica_source = sources.PhEDExReplicaInfoSource(config.replicas.config)

## Start the update
# 1. Refresh groups
# 2. Get the list of block replicas and dataset names to update
# 3. Update the datasets
# 4. Loop over new and changed block replicas, add them to inventory
# 5. Pick up deleted block replicas
# 6. Loop over deleted replicas
# 7. Save the execution state

from dynamo.dataformat import Dataset, Block, File, DatasetReplica, BlockReplica, ObjectError
from dynamo.utils.parallel import Map

parallelizer = Map()
parallelizer.timeout = 7200

## 1. Refresh groups

LOG.info('Updating list of groups.')
for group in group_source.get_group_list():
    LOG.debug('Updating %s', str(group))
    inventory.update(group)

## 2. Get the list of block replicas and dataset names to update

if args.mode == 'ReplicaDelta':
    ## Global delta-update of replicas
    ## Get the last update timestamp

    check_replicas = True

    if args.delta_since:
        last_update = args.delta_since

    else:
        state_db = sqlite3.connect(config.updater_state_file)
        cursor = state_db.cursor()
    
        sql = 'SELECT MAX(`timestamp`) FROM `replica_delta_updates`'
        result = cursor.execute(sql)
        last_update = next(result)[0]
    
        if last_update is None:
            LOG.error('Last update timestamp is not set. Run a full update of all sites and create a timestamp.')
            sys.exit(1)
    
        state_db.close()
    
        # Allow 30-minute safety margin to fully collect all updates
        # Yes we really need that much (have seen as far as 20 minute delay in blockreplicas)
        last_update -= 1800

    # The timestamp for this update
    update_start = time.time()
    
    ## Fetch the full list of block replicas that were updated since updated_since.
    ## New datasets and blocks will be caught in the process.
    
    timestamp_str = time.strftime('%Y-%m-%d %H:%M:%S %Z', time.localtime(last_update))
    LOG.info('Fetching the list of block replicas updated since %s.', timestamp_str)

    deleted_replicas = []

    # Fetch deleted replicas in a separate thread
    def get_deleted_replicas():
        replicas = replica_source.get_deleted_replicas(last_update)
        deleted_replicas.extend(replicas)

    thr = threading.Thread(target = get_deleted_replicas)
    thr.start()

    updated_replicas = replica_source.get_updated_replicas(last_update)

    thr.join()

    # Sometimes when a subscription is made but is removed before any transfer happens, PhEDEx "deletions" call
    # returns nothing and the empty subscription stays in the inventory, inflating the projected volume at the site.
    # We need to check all empty replicas with PhEDEx.
    lock = threading.Lock()
    def check_empty_replica(replica):
        if not replica_source.replica_exists_at_site(replica.site, replica.dataset):
            with lock:
                deleted_replicas.extend(replica.block_replicas)

    empty_replicas = []
    for site in inventory.sites.itervalues():
        for replica in site.dataset_replicas():
            if replica.size(physical = True) == 0:
                empty_replicas.append(replica)

    parallelizer.execute(check_empty_replica, empty_replicas)
        
    # Names of all the datasets to update
    dataset_names = set(br.block.dataset.name for br in updated_replicas)

elif args.mode == 'ReplicaFull':
    ## Round-robin update of a site-tier combination
    ## Get the combination to run on

    check_replicas = True

    state_db = sqlite3.connect(config.updater_state_file)
    cursor = state_db.cursor()

    updated_replicas = []
    site_dataset_combos = []

    sql = 'SELECT `site`, `tier` FROM `replica_full_updates` ORDER BY `id` ASC'
    result = cursor.execute(sql)

    while len(updated_replicas) < 3000:
        try:
            site, tier = next(result)
        except StopIteration:
            LOG.error('Round robin state table is empty. Run generate_dataset_list_cms first.')
            sys.exit(0)

        dataset = '/*/*/' + tier

        updated_replicas.extend(replica_source.get_replicas(site = site, dataset = dataset))
        site_dataset_combos.append((str(site), str(dataset))) # sqlite3 gives us unicode

    LOG.info('Performing full inventory update for combinations %s', site_dataset_combos)

    state_db.close()

    dataset_names = set(br.block.dataset.name for br in updated_replicas)

else:
    if args.dataset_only:
        check_replicas = False
        dataset_names.update(set(dataset_source.get_dataset_names(include = args.datasets)))
    else:
        check_replicas = True

        if args.sites:
            site_dataset_combos = []
            site_list = None
            for site_name in args.sites:
                if site_name.startswith('!'):
                    # remove matching site from list filled so far
                    site_pat = re.compile(fnmatch.translate(site_name[1:]))

                    ic = 0
                    while ic != len(site_dataset_combos):
                        if site_pat.match(site_dataset_combos[ic][0]):
                            site_dataset_combos.pop(ic)
                        else:
                            ic += 1

                    continue

                if '*' in site_name:
                    # expand site name already so that we don't download a humongous replica list
                    site_pat = re.compile(fnmatch.translate(site_name))

                    if site_list is None:
                        site_list = site_source.get_site_list()

                    for site in site_list:
                        if site_pat.match(site.name):
                            site_dataset_combos.append((site.name, None))

                else:
                    site_dataset_combos.append((site_name, None))
                        
        else:
            site_dataset_combos = [(None, None)]

        if args.datasets:
            site_dataset_combos = [(c[0], d) for d in args.datasets for c in site_dataset_combos]

        updated_replicas = []
        for site_name, dataset_name in site_dataset_combos:
            updated_replicas.extend(replica_source.get_replicas(site = site_name, dataset = dataset_name))

        dataset_names = set(br.block.dataset.name for br in updated_replicas)

# 3. Update the datasets

# 3.1. Query the dataset source (parallelize)

LOG.info('Updating dataset information.')

def get_dataset(name):
    LOG.info('Updating information for dataset %s', name)
    
    dataset_tmp = dataset_source.get_dataset(name, with_files = True)

    if dataset_tmp is None:
        LOG.error('Unknown dataset %s.', name)
        
    return dataset_tmp

dataset_tmps = parallelizer.execute(get_dataset, dataset_names, async = True)

def create_file(file_tmp, block):
    LOG.debug('Creating new file %s', file_tmp.lfn)
    lfile = File(file_tmp.lfn, block)
    lfile.copy(file_tmp)
    block.add_file(lfile)
    inventory.register_update(lfile)

def create_block(block_tmp, dataset):
    LOG.debug('Creating new block %s', block_tmp.full_name())
    block = Block(block_tmp.name, dataset)
    block.copy(block_tmp)
    dataset.blocks.add(block)
    inventory.register_update(block)

    # add files
    for file_tmp in block_tmp.files:
        create_file(file_tmp, block)

def create_dataset(dataset_tmp):
    LOG.debug('Creating new dataset %s', dataset_tmp.name)
    dataset = Dataset(dataset_tmp.name)
    dataset.copy(dataset_tmp)
    inventory.datasets.add(dataset)
    inventory.register_update(dataset)

    # add blocks
    for block_tmp in dataset_tmp.blocks:
        create_block(block_tmp, dataset)

watermark = 0
idat = 0

for dataset_tmp in dataset_tmps:
    if float(idat) / len(dataset_names) * 100. >= watermark:
        LOG.info('%d%% done..', watermark)
        watermark += 5

    idat += 1

    if dataset_tmp is None:
        continue

    # 3.2. Find the dataset or create new

    try:
        dataset = inventory.datasets[dataset_tmp.name]
    except KeyError:
        create_dataset(dataset_tmp)
        continue

    if dataset != dataset_tmp:
        LOG.debug('Updating dataset %s', dataset.name)
        dataset.copy(dataset_tmp)
        inventory.register_update(dataset)

    # 3.3. Update blocks

    existing_blocks = dict((b.name, b) for b in dataset.blocks)
    block_names_in_source = set()

    for block_tmp in dataset_tmp.blocks:
        block_names_in_source.add(block_tmp.name)

        try:
            block = existing_blocks[block_tmp.name]
        except KeyError:
            create_block(block_tmp, dataset)
            continue

        if block == block_tmp:
            # If num_files and size are identical, there is very little chance that the actual file list is different.
            # We skip the file update.
            continue
        else:
            LOG.debug('Updating block %s', block.full_name())
            block.copy(block_tmp)
            inventory.register_update(block)

        # 3.4. Update files

        existing_files = dict(((f.lfn, f) for f in block.files))
        file_names_in_source = set()

        for file_tmp in block_tmp.files:
            lfn = file_tmp.lfn
            file_names_in_source.add(lfn)

            try:
                lfile = existing_files[lfn]
            except KeyError:
                create_file(file_tmp, block)
                continue

            if lfile != file_tmp:
                LOG.debug('Updating file %s', lfile.lfn)
                lfile.copy(file_tmp)
                inventory.register_update(lfile)

        # 3.5. Delete excess files

        for lfn in (set(existing_files.iterkeys()) - file_names_in_source):
            LOG.debug('Deleting file %s', lfn)
            inventory.delete(existing_files[lfn])

    # 3.6. Delete excess blocks

    for block_name in (set(existing_blocks.iterkeys()) - block_names_in_source):
        LOG.debug('Deleting block %s', block_name)
        inventory.delete(existing_blocks[block_name])

LOG.info('100% done.')

if check_replicas:
    ## 4. Loop over new and changed block replicas, add them to inventory

    num_replicas = len(updated_replicas)
    LOG.info('Got %d block replicas to update.', num_replicas)

    # Save the embedded versions - we cannot query for "replicas deleted since X", so instead compare
    # what is already in the database to what we get from PhEDEx.
    embedded_updated_replicas = set()

    group = None
    site = None
    dataset = None
    dataset_replica = None

    for replica in updated_replicas:
        replica_str = str(replica)
    
        LOG.debug('Checking %s', replica_str)
    
        # 4.1. pick up replicas of known groups only

        if group is None or group.name != replica.group.name:
            try:
                group = inventory.groups[replica.group.name]
            except KeyError:
                LOG.debug('%s is owned by %s, which is not a tracked group.', replica_str, replica.group.name)
                continue
    
        # 4.2. Pick up replicas at known sites only

        if site is None or site.name != replica.site.name:
            try:
                site = inventory.sites[replica.site.name]
            except KeyError:
                LOG.debug('%s is at %s, which is not a tracked site.', replica_str, replica.site.name)
                continue

            dataset_replica = None
    
        # 4.3. Update the dataset info

        if dataset is None or dataset.name != replica.block.dataset.name:
            dataset_name = replica.block.dataset.name
            if not dataset_name.startswith('/') or dataset_name.count('/') != 3:
                continue

            # valid datasets should exist in the repository now
            dataset = inventory.datasets[dataset_name]
            dataset_replica = None

        # 4.4. Find the dataset replica

        if dataset_replica is None:
            dataset_replica = site.find_dataset_replica(dataset)
    
        if dataset_replica is None:
            # If not found, create a new replica and inject
            LOG.info('Creating new replica of %s at %s', dataset.name, site.name)
            dataset_replica = DatasetReplica(dataset, site)
    
            dataset.replicas.add(dataset_replica)
            site.add_dataset_replica(dataset_replica, add_block_replicas = False)
    
            inventory.register_update(dataset_replica)

        # 4.5. Find the block of the dataset
    
        block = dataset.find_block(replica.block.name)
    
        if block is None:
            LOG.error('Unknown block %s.', replica.block.full_name())
            continue
    
        # 4.6. Update the block replica
    
        block_replica = block.find_replica(site)
    
        if block_replica is None:
            LOG.debug('Creating new replica of %s at %s', block.full_name(), site.name)
            block_replica = BlockReplica(block, site, group)
            block_replica.copy(replica)
            # reset the group
            block_replica.group = group
    
            dataset_replica.block_replicas.add(block_replica)
            block.replicas.add(block_replica)
            site.add_block_replica(block_replica)
    
            inventory.register_update(block_replica)
    
        elif block_replica != replica:
            LOG.debug('Updating %s', replica_str)
            block_replica.copy(replica)
            inventory.register_update(block_replica)
            
        if args.mode != 'ReplicaDelta':
            embedded_updated_replicas.add(block_replica)
  
    ## 5. Pick up deleted block replicas

    # deleted_replicas in ReplicaDelta mode is fetched together with updated_replicas
    if args.mode != 'ReplicaDelta':
        # direct or ReplicaFull. Lucky we have args.site and args.dataset defined in ReplicaFull.
        # Replicas in inventory but not in updated_replicas are deleted
        deleted_replicas = []

        # find block replicas that exist in the database but not in embedded_updated_replicas
        # for each site & dataset combination
        for site_name, dataset_name in site_dataset_combos:
            if site_name:
                site_pat = re.compile(fnmatch.translate(site_name))
            else:
                site_pat = None
    
            if dataset_name:
                dataset_pat = re.compile(fnmatch.translate(dataset_name))
            else:
                dataset_pat = None
    
            for site in inventory.sites.itervalues():
                if site_pat and not site_pat.match(site.name):
                    continue
        
                for dataset_replica in site.dataset_replicas():
                    if dataset_pat and not dataset_pat.match(dataset_replica.dataset.name):
                        continue
        
                    for block_replica in dataset_replica.block_replicas:
                        if block_replica not in embedded_updated_replicas:
                            deleted_replicas.append(block_replica)
    
    ## 6. Loop over deleted replicas
    
    for replica in deleted_replicas:
        replica_str = str(replica)
    
        LOG.debug('Deleting %s', replica_str)
    
        # 6.1. pick up replicas of known groups only
    
        if replica.group.name not in inventory.groups:
            LOG.debug('%s is owned by %s, which is not a tracked group.', replica_str, replica.group.name)
            continue
    
        # 6.2. Pick up replicas at known sites only
    
        try:
            site = inventory.sites[replica.site.name]
        except KeyError:
            LOG.debug('%s is at %s, which is not a tracked site.', replica_str, replica.site.name)
            continue
    
        # 6.3. Find the dataset in the inventory
    
        try:
            dataset = inventory.datasets[replica.block.dataset.name]
        except KeyError:
            # If not found, create a new dataset and inject
            LOG.debug('Unknown dataset %s.', replica.block.dataset.name)
            continue
    
        # 6.4. Find the block of the dataset
    
        block_full_name = replica.block.full_name()
        block = dataset.find_block(replica.block.name)
    
        if block is None:
            # If not found, create a new block and inject
            LOG.debug('Unknown block %s.', block_full_name)
            continue
    
        # 6.5. Find the dataset replica
    
        dataset_replica = site.find_dataset_replica(dataset)
        if dataset_replica is None:
            LOG.debug('No replica of %s at %s.', dataset.name, site.name)
            continue
    
        # 6.6. Delete the block replica
    
        # blockreplica.unlink_from() raises a KeyError or ObjectError if
        # any of the group, site, dataset, ... is not found
        # Containing dataset replica will be deleted within blockreplica.unlink()
        # if it becomes empty
        try:
            inventory.delete(replica)
        except (KeyError, ObjectError):
            LOG.debug('Replica not found.')
            pass

# 7. Save the execution state

if not read_only and args.mode is not None and os.path.exists(config.updater_state_file):
    # Regardless of update mode, clear the updated datasets from the table
    state_db = sqlite3.connect(config.updater_state_file)
    cursor = state_db.cursor()

    # Additionally for replica updates
    if args.mode == 'ReplicaDelta':
        sql = 'INSERT INTO `replica_delta_updates` VALUES (?, ?, ?)'
        cursor.execute(sql, (update_start, len(updated_replicas), len(deleted_replicas)))

    elif args.mode == 'ReplicaFull':
        for site_name, dataset_name in site_dataset_combos:
            tier = dataset_name[dataset_name.rfind('/') + 1:]
            cursor.execute('DELETE FROM `replica_full_updates` WHERE `site` = ? AND `tier` = ?', (site_name, tier))

    state_db.commit()
    state_db.close()

LOG.info('Inventory update completed.')
