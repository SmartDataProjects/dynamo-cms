#!/usr/bin/env python

import os
import sys
import time
import fnmatch
import sqlite3
import re
import threading
from argparse import ArgumentParser

parser = ArgumentParser(description = 'Update replica information.')
parser.add_argument('--config', '-c', metavar = 'CONFIG', dest = 'config', required = True, help = 'Configuration JSON.')
parser.add_argument('--mode', '-m', metavar = 'MODE', dest = 'mode', help = 'Automatic update mode. Values are: ReplicaDelta, ReplicaFull. When set, --site and --dataset are ignored.')
parser.add_argument('--site', '-s', metavar = 'SITE', dest = 'sites', nargs = '+', help = 'Do a full update of the site (wildcard allowed). Prepend ! to veto match.')
parser.add_argument('--dataset', '-d', metavar = 'DATASET', dest = 'datasets', nargs = '+', help = 'Do a full update of the dataset (wildcard allowed).')
parser.add_argument('--delta-since', '-t', metavar = 'TIME', dest = 'delta_since', type = int, help = 'With --mode=ReplicaDelta, override the update_since timestamp.')

args = parser.parse_args()
sys.argv = []

## Load the configuration
from dynamo.core.executable import make_standard_logger, authorized, inventory
from dynamo.dataformat import Dataset, Block, File, Group, DatasetReplica, BlockReplica, ObjectError
from dynamo.dataformat.configuration import Configuration
from dynamo.dataformat.history import DeletedReplica, CopiedReplica, HistoryRecord
from dynamo.source.impl.phedexgroupinfo import PhEDExGroupInfoSource
from dynamo.source.impl.phedexsiteinfo import PhEDExSiteInfoSource
from dynamo.source.impl.phedexdatasetinfo import PhEDExDatasetInfoSource
from dynamo.source.impl.phedexreplicainfo import PhEDExReplicaInfoSource
from dynamo.operation.impl.phedexcopy import PhEDExCopyInterface
from dynamo.operation.impl.phedexdeletion import PhEDExDeletionInterface
from dynamo.operation.history import DeletionHistoryDatabase, CopyHistoryDatabase
from dynamo.fileop.rlfsm import RLFSM
from dynamo.registry.registry import RegistryDatabase
from dynamo.utils.interface.mysql import MySQL
from dynamo.utils.interface.phedex import PhEDEx
from dynamo.utils.parallel import Map

config = Configuration(args.config)

## Set up logging (write to stdout & stderr)

LOG = make_standard_logger(config.get('log_level', 'info'))

## Option compatibilities
if args.mode not in [None, 'ReplicaDelta', 'ReplicaFull', 'NoPhEDEx']:
    LOG.error('Unknown automatic update mode %s.', args.mode)
    sys.exit(1)

if args.mode is None and args.sites is None and args.datasets is None:
    LOG.error('At least one of --site --dataset is needed for non-automatic update.')
    sys.exit(1)

if args.mode in ['ReplicaDelta', 'ReplicaFull'] and not os.path.exists(config.updater_state_file):
    LOG.error('State file %s does not exist. Run generate_dataset_list_cms first.', config.replica_state_file)
    sys.exit(1)

LOG.info('Starting inventory update.')

## Load and initialize sources

if 'sites' not in config:
    config.sites = Configuration()
if 'datasets' not in config:
    config.datasets = Configuration()
if 'replicas' not in config:
    config.replicas = Configuration()

if 'include_sites' in config:
    config.sites.include = config.include_sites
    config.replicas.include_sites = config.include_sites
if 'exclude_sites' in config:
    config.sites.exclude = config.exclude_sites
    config.replicas.exclude_sites = config.exclude_sites
if 'include_datasets' in config:
    config.datasets.include = config.include_datasets
    config.replicas.include_datasets = config.include_datasets
if 'exclude_datasets' in config:
    config.datasets.exclude = config.exclude_datasets
    config.replicas.exclude_datasets = config.exclude_datasets

group_source = PhEDExGroupInfoSource(config.get('groups', None))
site_source = PhEDExSiteInfoSource(config.sites)
dataset_source = PhEDExDatasetInfoSource(config.datasets)
replica_source = PhEDExReplicaInfoSource(config.replicas)

## Tools for file-level operations

rlfsm = RLFSM()
phedex_copy = PhEDExCopyInterface()
phedex_deletion = PhEDExDeletionInterface()
copy_history = CopyHistoryDatabase()
deletion_history = DeletionHistoryDatabase()
phedex = PhEDEx()

if not authorized:
    rlfsm.set_read_only()
    phedex_copy.set_read_only()
    phedex_deletion.set_read_only()
    copy_history.set_read_only()
    deletion_history.set_read_only()

# For local invalidation noticies
registry = RegistryDatabase()

## Reservations DB (keeping track of in-house operations we did)

reservations_db = MySQL(config.reservations_db_params)

transfer_reservation_entries = reservations_db.query('SELECT `id`, `operation_id`, `item`, `site`, `group` FROM `phedex_transfer_reservations`')

# Reserved transfer should be canceled if there is already a subscription
def find_subscription(rid, opid, item_name, site_name, group_name):
    try:
        Block.from_full_name(item_name)
    except ObjectError:
        is_block = False
    else:
        is_block = True

    options = ['node=' + site_name, 'group=' + group_name]
    if is_block:
        options.append('block=' + item_name)
    else:
        options.append('dataset=' + item_name)

    dataset_entries = phedex.make_request('subscriptions', options)

    return item_name, site_name, (len(dataset_entries) != 0)

subscription_results = Map().execute(find_subscription, transfer_reservation_entries)

existing_subscriptions = set((item_name, site_name) for item_name, site_name, exists in subscription_results if exists)

# Pre-fetch the list of reserved transfers before we deal with deletions
transfer_reservations = {} # {dataset_replica: (set(blocks), group, reservation id, operation id)}

for rid, opid, item_name, site_name, group_name in transfer_reservation_entries:
    try:
        dataset_name, block_name = Block.from_full_name(item_name)
    except ObjectError:
        dataset_name = item_name
        block_name = None

    try:
        dataset = inventory.datasets[dataset_name]
        site = inventory.sites[site_name]
        group = inventory.groups[group_name]
    except KeyError:
        # shouldn't happen
        if authorized:
            reservations_db.query('DELETE FROM `phedex_transfer_reservations` WHERE `id` = %s', rid)
        continue

    replica = site.find_dataset_replica(dataset)
    if replica is None:
        # shouldn't happen but we allow it
        replica = inventory.update(DatasetReplica(dataset, site))

    if block_name is None:
        if not replica.growing or replica.group is not group:
            replica.growing = True
            replica.group = group
            inventory.register_update(replica)

        blocks = set()
        for block in dataset.blocks:
            if block.find_replica(site) is None:
                inventory.update(BlockReplica(block, site, group, size = 0))

            blocks.add(block)
    else:
        block = dataset.find_block(block_name)
        if block is None:
            # shouldn't happen
            if authorized:
                reservations_db.query('DELETE FROM `phedex_transfer_reservations` WHERE `id` = %s', rid)
            continue

        if block.find_replica(site) is None:
            inventory.update(BlockReplica(block, site, group, size = 0))

        blocks = set([block])

    if (item_name, site_name) in existing_subscriptions:
        LOG.info('PhEDEx subscription of %s at %s already exists. Deleting the reservation and canceling the subscriptions.', item_name, site_name)
        if authorized:
            reservations_db.query('DELETE FROM `phedex_transfer_reservations` WHERE `id` = %s', rid)
        for block in blocks:
            for lfile in block.files:
                rlfsm.cancel_subscription(site, lfile)

        continue

    transfer_reservations[replica] = (blocks, group, rid, opid)

# Now for the deletions
deletion_reservation_entries = reservations_db.query('SELECT `id`, `operation_id`, `item`, `site` FROM `phedex_deletion_reservations`')

deletion_reservations = {}

for rid, opid, item_name, site_name in deletion_reservation_entries:
    try:
        dataset_name, block_name = Block.from_full_name(item_name)
    except ObjectError:
        dataset_name = item_name
        block_name = None

    try:
        dataset = inventory.datasets[dataset_name]
        site = inventory.sites[site_name]
    except KeyError:
        # shouldn't happen
        if authorized:
            reservations_db.query('DELETE FROM `phedex_deletion_reservations` WHERE `id` = %s', rid)
        continue

    replica = site.find_dataset_replica(dataset)
    if replica is None:
        # shouldn't happen but we allow it
        replica = inventory.update(DatasetReplica(dataset, site))

    if block_name is None:
        blocks = set()
        for block in dataset.blocks:
            if block.find_replica(site) is None:
                inventory.update(BlockReplica(block, site, group, size = 0))

            blocks.add(block)
    else:
        block = dataset.find_block(block_name)
        if block is None:
            # shouldn't happen
            if authorized:
                reservations_db.query('DELETE FROM `phedex_deletion_reservations` WHERE `id` = %s', rid)
            continue

        if block.find_replica(site) is None:
            inventory.update(BlockReplica(block, site, group, size = 0))

        blocks = set([block])

    deletion_reservations[replica] = (blocks, rid, opid)
    

if args.mode != 'NoPhEDEx':
    ## Start the update
    # 1. Refresh groups
    # 2. Get the list of block replicas and dataset names to update
    # 3. Update the datasets
    # 4. Loop over new and changed block replicas, add them to inventory
    # 5. Pick up deleted block replicas
    # 6. Loop over deleted replicas
    # 7. Set growing flag for dataset replicas
    # 8. Save the execution state
    
    parallelizer = Map()
    parallelizer.timeout = 7200
    
    ## 1. Refresh groups
    
    LOG.info('Updating list of groups.')
    for group in group_source.get_group_list():
        LOG.debug('Updating %s', str(group))
        inventory.update(group)
    
    ## 2. Get the list of block replicas and dataset names to update
    
    if args.mode == 'ReplicaDelta':
        ## Global delta-update of replicas
        ## Get the last update timestamp
    
        if args.delta_since:
            last_update = args.delta_since
    
        else:
            state_db = sqlite3.connect(config.updater_state_file)
            cursor = state_db.cursor()
        
            sql = 'SELECT MAX(`timestamp`) FROM `replica_delta_updates`'
            result = cursor.execute(sql)
            last_update = next(result)[0]
        
            if last_update is None:
                LOG.error('Last update timestamp is not set. Run a full update of all sites and create a timestamp.')
                sys.exit(1)
        
            state_db.close()
        
            # Allow 30-minute safety margin to fully collect all updates
            # Yes we really need that much (have seen as far as 20 minute delay in blockreplicas)
            last_update -= 1800
    
        # The timestamp for this update
        update_start = time.time()
        
        ## Fetch the full list of block replicas that were updated since updated_since.
        ## New datasets and blocks will be caught in the process.
        
        timestamp_str = time.strftime('%Y-%m-%d %H:%M:%S %Z', time.localtime(last_update))
        LOG.info('Fetching the list of block replicas updated since %s.', timestamp_str)
    
        deleted_replicas = []
    
        # Fetch deleted replicas in a separate thread
        def get_deleted_replicas():
            replicas = replica_source.get_deleted_replicas(last_update)
            deleted_replicas.extend(replicas)
    
        thr = threading.Thread(target = get_deleted_replicas)
        thr.start()
    
        updated_replicas = replica_source.get_updated_replicas(last_update, inventory)
    
        thr.join()
    
        # Sometimes when a subscription is made but is removed before any transfer happens, PhEDEx "deletions" call
        # returns nothing and the empty subscription stays in the inventory, inflating the projected volume at the site.
        # We need to check all empty replicas with PhEDEx.
        def check_empty_replica(replica):
            return replica, replica_source.replica_exists_at_site(replica.site, replica.dataset)
    
        empty_replicas = []
        for site in inventory.sites.itervalues():
            for replica in site.dataset_replicas():
                if replica.size(physical = True) == 0:
                    empty_replicas.append(replica)
    
        checks = parallelizer.execute(check_empty_replica, empty_replicas, async = True)
    
        for replica, exists in checks:
            if not exists:
                deleted_replicas.extend(replica.block_replicas)
            
        # Names of all the datasets to update
        dataset_names = set(br.block.dataset.name for br in updated_replicas)
    
    elif args.mode == 'ReplicaFull':
        ## Round-robin update of a site-tier combination
        ## Get the combination to run on
    
        state_db = sqlite3.connect(config.updater_state_file)
        cursor = state_db.cursor()
    
        updated_replicas = []
        site_dataset_combos = []
    
        sql = 'SELECT `site`, `tier` FROM `replica_full_updates` ORDER BY `id` ASC'
        result = cursor.execute(sql)
    
        while len(updated_replicas) < 3000:
            try:
                site, tier = next(result)
            except StopIteration:
                LOG.error('Round robin state table is empty. Run generate_dataset_list_cms first.')
                sys.exit(0)
    
            dataset = '/*/*/' + tier
    
            updated_replicas.extend(replica_source.get_replicas(site = site, dataset = dataset))
            site_dataset_combos.append((str(site), str(dataset))) # sqlite3 gives us unicode
    
        LOG.info('Performing full inventory update for combinations %s', site_dataset_combos)
    
        state_db.close()
    
        dataset_names = set(br.block.dataset.name for br in updated_replicas)
    
    else:
        if args.sites:
            sites = []
            site_list = None
            for site_name in args.sites:
                if site_name.startswith('!'):
                    # remove matching site from list filled so far
                    site_pat = re.compile(fnmatch.translate(site_name[1:]))
    
                    ic = 0
                    while ic != len(sites):
                        if site_pat.match(sites[ic]):
                            sites.pop(ic)
                        else:
                            ic += 1
    
                    continue
    
                if '*' in site_name:
                    # expand site name already so that we don't download a humongous replica list
                    site_pat = re.compile(fnmatch.translate(site_name))
    
                    if site_list is None:
                        site_list = site_source.get_site_list()
    
                    for site in site_list:
                        if site_pat.match(site.name):
                            sites.append(site.name)
    
                else:
                    sites.append(site_name)
                        
        else:
            sites = [None]
    
        if args.datasets:
            site_dataset_combos = [(c, d) for d in args.datasets for c in sites]
        else:
            # parallelize by tier (also tried splitting by acquisition era, but there are too many)
            result = dataset_source._dbs.make_request('datatiers')
            tiers = [entry['data_tier_name'] for entry in result if entry['data_tier_name'] not in ('DAVE', 'CRAP', 'DBS3_DEPLOYMENT_TEST_TIER')]
            site_dataset_combos = [(c, '/*/*/%s' % tier) for tier in tiers for c in sites]
    
        def add_updated_replicas(site_name, dataset_name):
            return replica_source.get_replicas(site = site_name, dataset = dataset_name)
    
        replica_lists = parallelizer.execute(add_updated_replicas, site_dataset_combos, async = True)
    
        updated_replicas = []
        for replica_list in replica_lists:
            updated_replicas.extend(replica_list)
    
        dataset_names = set(br.block.dataset.name for br in updated_replicas)
    
    # 3. Update the datasets
    
    # 3.1. Query the dataset source (parallelize)
    
    LOG.info('Updating dataset information.')
    
    def get_dataset(name):
        LOG.info('Updating information for dataset %s', name)
        
        dataset_tmp = dataset_source.get_dataset(name, with_files = True)
    
        if dataset_tmp is None:
            LOG.error('Unknown dataset %s.', name)
            
        return dataset_tmp
    
    dataset_tmps = parallelizer.execute(get_dataset, dataset_names, async = True)
    
    def create_file(file_tmp, block):
        LOG.debug('Creating new file %s', file_tmp.lfn)
        lfile = File(file_tmp.lfn, block)
        lfile.copy(file_tmp)
        block.add_file(lfile)
        inventory.register_update(lfile)
    
    def create_block(block_tmp, dataset):
        LOG.debug('Creating new block %s', block_tmp.full_name())
        block = Block(block_tmp.name, dataset)
        block.copy(block_tmp)
        dataset.blocks.add(block)
        inventory.register_update(block)
    
        # add files
        for file_tmp in block_tmp.files:
            create_file(file_tmp, block)
    
    def create_dataset(dataset_tmp):
        LOG.debug('Creating new dataset %s', dataset_tmp.name)
        dataset = Dataset(dataset_tmp.name)
        dataset.copy(dataset_tmp)
        inventory.datasets.add(dataset)
        inventory.register_update(dataset)
    
        # add blocks
        for block_tmp in dataset_tmp.blocks:
            create_block(block_tmp, dataset)
    
    watermark = 0
    idat = 0
    
    for dataset_tmp in dataset_tmps:
        if float(idat) / len(dataset_names) * 100. >= watermark:
            LOG.info('%d%% done..', watermark)
            watermark += 5
    
        idat += 1
    
        if dataset_tmp is None:
            continue
    
        # 3.2. Find the dataset or create new
    
        try:
            dataset = inventory.datasets[dataset_tmp.name]
        except KeyError:
            create_dataset(dataset_tmp)
            continue
    
        if dataset != dataset_tmp:
            LOG.debug('Updating dataset %s', dataset.name)
            dataset.copy(dataset_tmp)
            inventory.register_update(dataset)
    
        # 3.3. Update blocks
    
        existing_blocks = dict((b.name, b) for b in dataset.blocks)
        block_names_in_source = set()
    
        for block_tmp in dataset_tmp.blocks:
            block_names_in_source.add(block_tmp.name)
    
            try:
                block = existing_blocks[block_tmp.name]
            except KeyError:
                create_block(block_tmp, dataset)
                continue
    
            if block == block_tmp:
                # If num_files and size are identical, there is very little chance that the actual file list is different.
                # We skip the file update.
                continue
            else:
                LOG.debug('Updating block %s', block.full_name())
                block.copy(block_tmp)
                inventory.register_update(block)
    
            # 3.4. Update files
    
            existing_files = dict(((f.lfn, f) for f in block.files))
            file_names_in_source = set()
    
            for file_tmp in block_tmp.files:
                lfn = file_tmp.lfn
                file_names_in_source.add(lfn)
    
                try:
                    lfile = existing_files[lfn]
                except KeyError:
                    create_file(file_tmp, block)
                    continue
    
                if lfile != file_tmp:
                    LOG.debug('Updating file %s', lfile.lfn)
                    lfile.copy(file_tmp)
                    inventory.register_update(lfile)
    
            # 3.5. Delete excess files
    
            for lfn in (set(existing_files.iterkeys()) - file_names_in_source):
                LOG.debug('Deleting file %s', lfn)
                inventory.delete(existing_files[lfn])
    
        # 3.6. Delete excess blocks
    
        for block_name in (set(existing_blocks.iterkeys()) - block_names_in_source):
            LOG.debug('Deleting block %s', Block.to_full_name(dataset.name, Block.to_real_name(block_name)))
            inventory.delete(existing_blocks[block_name])

            # take this block out of reservations
            block = existing_blocks[block_name]
            block_full_name = block.full_name()

            for replica, reservation in transfer_reservations.iteritems():
                if replica.dataset is not block.dataset:
                    continue
                try:
                    reservation[0].remove(block)
                except KeyError:
                    pass

            for rid, opid, item_name, site_name, group_name in transfer_reservation_entries:
                if item_name == block_full_name:
                    if authorized:
                        reservations_db.query('DELETE FROM `phedex_transfer_reservations` WHERE `id` = %s', rid)

            for replica, reservation in deletion_reservations.iteritems():
                if replica.dataset is not block.dataset:
                    continue
                try:
                    reservation[0].remove(block)
                except KeyError:
                    pass

            for rid, opid, item_name, site_name, group_name in deletion_reservation_entries:
                if item_name == block_full_name:
                    if authorized:
                        reservations_db.query('DELETE FROM `phedex_deletion_reservations` WHERE `id` = %s', rid)
    
    LOG.info('100% done.')
    
    ## 4. Loop over new and changed block replicas, add them to inventory
    
    num_replicas = len(updated_replicas)
    LOG.info('Got %d block replicas to update.', num_replicas)
    
    # Save the embedded versions - we cannot query for "replicas deleted since X", so instead compare
    # what is already in the database to what we get from PhEDEx.
    embedded_updated_replicas = set()
    
    group = None
    site = None
    dataset = None
    dataset_replica = None
    
    for replica in updated_replicas:
        replica_str = str(replica)
    
        LOG.debug('Checking %s', replica_str)
    
        # 4.1. Pick up replicas of known groups only
    
        if group is None or group.name != replica.group.name:
            try:
                group = inventory.groups[replica.group.name]
            except KeyError:
                LOG.debug('%s is owned by %s, which is not a tracked group.', replica_str, replica.group.name)
                continue
    
        # 4.2. Pick up replicas at known sites only
    
        if site is None or site.name != replica.site.name:
            try:
                site = inventory.sites[replica.site.name]
            except KeyError:
                LOG.debug('%s is at %s, which is not a tracked site.', replica_str, replica.site.name)
                continue
    
            dataset_replica = None
    
        # 4.3. Update the dataset info
    
        if dataset is None or dataset.name != replica.block.dataset.name:
            dataset_name = replica.block.dataset.name
            if not dataset_name.startswith('/') or dataset_name.count('/') != 3:
                continue
    
            # valid datasets should exist in the repository now
            dataset = inventory.datasets[dataset_name]
            dataset_replica = None
    
        # 4.4. Find the dataset replica
    
        if dataset_replica is None:
            dataset_replica = site.find_dataset_replica(dataset)
    
        if dataset_replica is None:
            # If not found, create a new replica and inject
            LOG.info('Creating new replica of %s at %s', dataset.name, site.name)
            dataset_replica = DatasetReplica(dataset, site)
    
            dataset.replicas.add(dataset_replica)
            site.add_dataset_replica(dataset_replica, add_block_replicas = False)
    
            inventory.register_update(dataset_replica)
    
        # 4.5. Find the block of the dataset
    
        block = dataset.find_block(replica.block.name)
    
        if block is None:
            LOG.error('Unknown block %s.', replica.block.full_name())
            continue
   
        # 4.6. Update the block replica
    
        block_replica = block.find_replica(site)
    
        if block_replica is None:
            LOG.debug('Creating new replica of %s at %s', block.full_name(), site.name)
            block_replica = BlockReplica(block, site, group)
            block_replica.copy(replica)
            # reset the group
            block_replica.group = group
    
            dataset_replica.block_replicas.add(block_replica)
            block.replicas.add(block_replica)
            site.add_block_replica(block_replica)
    
            inventory.register_update(block_replica)
    
        else:
            if block_replica != replica:
                LOG.debug('Updating %s', replica_str)
                block_replica.copy(replica)
                inventory.register_update(block_replica)
            
        if args.mode != 'ReplicaDelta':
            embedded_updated_replicas.add(block_replica)
    
    ## 5. Pick up deleted block replicas
    
    # deleted_replicas in ReplicaDelta mode is fetched together with updated_replicas
    if args.mode != 'ReplicaDelta':
        # direct or ReplicaFull. Lucky we have args.site and args.dataset defined in ReplicaFull.
        # Replicas in inventory but not in updated_replicas are deleted
        deleted_replicas = []
    
        # find block replicas that exist in the database but not in embedded_updated_replicas
        # for each site & dataset combination
        for site_name, dataset_name in site_dataset_combos:
            if site_name:
                site_pat = re.compile(fnmatch.translate(site_name))
            else:
                site_pat = None
    
            if dataset_name:
                dataset_pat = re.compile(fnmatch.translate(dataset_name))
            else:
                dataset_pat = None
    
            for site in inventory.sites.itervalues():
                if site_pat and not site_pat.match(site.name):
                    continue
        
                for dataset_replica in site.dataset_replicas():
                    if dataset_pat and not dataset_pat.match(dataset_replica.dataset.name):
                        continue
        
                    for block_replica in dataset_replica.block_replicas:
                        if block_replica not in embedded_updated_replicas:
                            deleted_replicas.append(block_replica)
    
    ## 6. Loop over deleted replicas
    
    for replica in deleted_replicas:
        replica_str = str(replica)
    
        LOG.debug('Deleting %s', replica_str)
    
        # 6.1. Pick up replicas of known groups only
    
        if replica.group.name not in inventory.groups:
            LOG.debug('%s is owned by %s, which is not a tracked group.', replica_str, replica.group.name)
            continue
    
        # 6.2. Pick up replicas at known sites only
    
        try:
            site = inventory.sites[replica.site.name]
        except KeyError:
            LOG.debug('%s is at %s, which is not a tracked site.', replica_str, replica.site.name)
            continue
    
        # 6.3. Find the dataset in the inventory
    
        try:
            dataset = inventory.datasets[replica.block.dataset.name]
        except KeyError:
            # If not found, create a new dataset and inject
            LOG.debug('Unknown dataset %s.', replica.block.dataset.name)
            continue

        # 6.4. Find the dataset replica
    
        dataset_replica = site.find_dataset_replica(dataset)
        if dataset_replica is None:
            LOG.debug('No replica of %s at %s.', dataset.name, site.name)
            continue
    
        # A reserved replica can appear in the deleted_replicas list if we are doing a full update
        if dataset_replica in transfer_reservations:
            transfer_reservation = transfer_reservations[dataset_replica]
        else:
            transfer_reservation = None

        if dataset_replica in deletion_reservations:
            deletion_reservation = deletion_reservations[dataset_replica]
        else:
            deletion_reservation = None
    
        # 6.5. Find the block of the dataset
    
        block_full_name = replica.block.full_name()
        block = dataset.find_block(replica.block.name)
    
        if block is None:
            # If not found, create a new block and inject
            LOG.debug('Unknown block %s.', block_full_name)
            continue
    
        if transfer_reservation is not None and block in transfer_reservation[0]:
            # We don't delete this block rep from the inventory
            continue

        if deletion_reservation is not None and block in deletion_reservation[0]:
            # We don't delete this block rep from the inventory
            continue
   
        # 6.6. Delete the block replica
    
        # blockreplica.unlink_from() raises a KeyError or ObjectError if
        # any of the group, site, dataset, ... is not found
        # Containing dataset replica will be deleted within blockreplica.unlink()
        # if it becomes empty
        try:
            inventory.delete(replica)
        except (KeyError, ObjectError):
            LOG.debug('Replica not found.')
            pass
    
    # 7. Set growing flag for dataset replicas
    
    for dataset in inventory.datasets.itervalues():
        if len(dataset.blocks) == 0:
            continue
    
        for replica in dataset.replicas:
            if len(replica.block_replicas) == len(dataset.blocks):
                if not replica.growing:
                    all_blocks_subscribed = True
                    for br in replica.block_replicas:
                        if br.group is Group.null_group:
                            all_blocks_subscribed = False
                            break

                    if all_blocks_subscribed:
                        replica.growing = True
                        replica.group = list(replica.block_replicas)[0].group
                        inventory.register_update(replica)
            else:
                if replica.growing:
                    replica.growing = False
                    replica.group = Group.null_group
                    inventory.register_update(replica)
    
    # 8. Save the execution state (at the end of the script)
    
#close if args.mode != 'NoPhEDEx':

## Deal with Dynamo-run transfers and deletions

LOG.info('Collecting in-house transfers and deletions.')

subscriptions = rlfsm.get_subscriptions(inventory, status = ['done', 'new', 'inbatch', 'retry', 'held'])

done_subscriptions = {}
done_desubscriptions = {}
subscribed_files = {}
desubscribed_files = {}

for sub in subscriptions:
    if type(sub) is RLFSM.Subscription:
        key = (sub.file.block, sub.destination)

        if sub.status == 'done':
            try:
                done_subscriptions[key].append(sub)
            except KeyError:
                done_subscriptions[key] = [sub]

        try:
            subscribed_files[key].append(sub.file.id)
        except KeyError:
            subscribed_files[key] = [sub.file.id]

    else:
        key = (sub.file.block, sub.site)

        if sub.status == 'done':
            try:
                done_desubscriptions[key].append(sub)
            except KeyError:
                done_desubscriptions[key] = [sub]

        try:
            desubscribed_files[key].append(sub.file.id)
        except KeyError:
            desubscribed_files[key] = [sub.file.id]

completed_subscriptions = {}

for key, subs in done_subscriptions.iteritems():
    block, site = key
    replica = block.find_replica(site)

    # replica must exist (guaranteed by RLFSM)
    if replica.file_ids is None:
        # is already full
        pass

    else:
        file_ids = set(sub.file.id for sub in subs)
        file_ids.update(replica.file_ids)
        all_file_ids = set()
        for f in block.files:
            if f.id == 0:
                all_file_ids.add(f.lfn)
            else:
                all_file_ids.add(f.id)

        if all_file_ids != set(file_ids):
            # block replica is not complete
            continue

    completed_subscriptions[key] = [sub.id for sub in subs]

completed_desubscriptions = {}

for key, subs in done_desubscriptions.iteritems():
    block, site = key
    replica = block.find_replica(site)

    if replica.file_ids is None:
        file_ids = set(f.id for f in block.files)
    else:
        file_ids = set(replica.file_ids)

    if set(sub.file.id for sub in subs) >= file_ids:
        completed_desubscriptions[key] = [sub.id for sub in subs]

# Completed_subscriptions and completed_desubscriptions now contain (block, site) => [subscription ids] of (de)subscriptions
# that are ready to be reported to PhEDEx

done_subscription_ids = []

# First deal with copies

done_transfer_reservation_ids = []
copy_replica_list = {} # {operation id: [datasetreplica]} dataset replica is a clone

for dataset_replica, (blocks, group, rid, opid) in transfer_reservations.iteritems():
    dataset = dataset_replica.dataset
    site = dataset_replica.site

    # Remove the list of files from done_subscriptions -> remaining files in the list are non-reserved and should be used to simply update the inventory
    for block in blocks:
        try:
            done_subscriptions.pop((block, site))
        except KeyError:
            pass

    dataset_done_ids = []
    for block in blocks:
        try:
            dataset_done_ids.extend(completed_subscriptions[(block, site)])
        except KeyError:
            break

    else:
        # All blocks completed
        done_subscription_ids.extend(dataset_done_ids)
        done_transfer_reservation_ids.append(rid)

        if opid not in copy_replica_list:
            copy_replica_list[opid] = []

        try:
            replica = next(r for r in copy_replica_list[opid] if r.dataset is dataset)
        except StopIteration:
            replica = DatasetReplica(dataset, site, growing = False)
            copy_replica_list[opid].append(replica)

        # blockreplicas below are created with size = 0 because PhEDEx must ultimately confirm the replica and report back

        if blocks == dataset.blocks:
            # dataset replica reserved
            replica.growing = True
            replica.group = group

        elif replica.growing:
            continue

        for block in blocks:
            replica.block_replicas.add(BlockReplica(block, site, group, size = 0))

        continue

    # Reservation is not complete - check that we have all necessary file subscriptions to complete this reservation eventually
    for block in blocks:
        replica = block.find_replica(site) # replica must exist
        if replica.file_ids is None:
            # no missing file
            continue

        missing_files = []
        for lfile in block.files:
            if not replica.has_file(lfile):
                missing_files.append(lfile)

        n = 0
        try:
            subscribed_file_ids = set(subscribed_files[(block, site)])
        except KeyError:
            for lfile in missing_files:
                n += 1
                rlfsm.subscribe_file(site, lfile)
        else:
            for lfile in missing_files:
                if lfile.id not in subscribed_file_ids:
                    n += 1
                    rlfsm.subscribe_file(site, lfile)

        if n != 0:
            LOG.info('Created %d file subscriptions for %s at %s', n, block.full_name(), site.name)
            
# Then deal with deletions
# Next block follows very similar logic to transfer reservations - see above for descriptions

done_deletion_reservation_ids = []
deletion_replica_list = {} # {operation id: {datasetreplica: [blockreplicas]}}

for dataset_replica, (blocks, rid, opid) in deletion_reservations.iteritems():
    dataset = dataset_replica.dataset
    site = dataset_replica.site

    for block in blocks:
        try:
            done_desubscriptions.pop((block, site))
        except KeyError:
            pass

    dataset_done_ids = []
    for block in blocks:
        replica = block.find_replica(site)
        if replica is None or (replica.file_ids is not None and len(replica.file_ids) == 0):
            continue

        try:
            dataset_done_ids.extend(completed_desubscriptions[(block, site)])
        except KeyError:
            break

    else:
        # All blocks completed
        done_subscription_ids.extend(dataset_done_ids)
        done_deletion_reservation_ids.append(rid)

        if opid not in deletion_replica_list:
            deletion_replica_list[opid] = {}

        if blocks == dataset.blocks:
            deletion_replica_list[opid][dataset_replica] = None
        elif dataset_replica not in deletion_replica_list[opid]:
            deletion_replica_list[opid][dataset_replica] = [BlockReplica(block, site, Group.null_group) for block in blocks]
        elif deletion_replica_list[opid][dataset_replica] is not None:
            deletion_replica_list[opid][dataset_replica].extend(BlockReplica(block, site, Group.null_group) for block in blocks)

        continue

    # Check that we have all necessary file desubscriptions to complete this reservation eventually
    for block in blocks:
        replica = block.find_replica(site) # replica must exist
        if replica.file_ids is None:
            existing_files = list(block.files)
        elif len(replica.file_ids) == 0:
            continue
        else:
            existing_files = []
            for lfile in block.files:
                if replica.has_file(lfile):
                    existing_files.append(lfile)

        n = 0
        try:
            desubscribed_file_ids = set(desubscribed_files[(block, site)])
        except KeyError:
            for lfile in existing_files:
                n += 1
                rlfsm.desubscribe_file(site, lfile)
        else:
            for lfile in existing_files:
                if lfile.id not in desubscribed_file_ids:
                    n += 1
                    rlfsm.desubscribe_file(site, lfile)
        if n != 0:
            LOG.warning('Recovered %d file desubscriptions for %s at %s', n, block.full_name(), site.name)

# Execute on PhEDEx
for opid, replicas in copy_replica_list.iteritems():
    scheduled_replicas = phedex_copy.schedule_copies(replicas, opid)

    for replica in scheduled_replicas:
        inventory.update(replica)
        for block_replica in replica.block_replicas:
            inventory.update(block_replica)

for opid, replicas in deletion_replica_list.iteritems():
    scheduled_replicas = phedex_deletion.schedule_deletions(replicas.items(), opid)

    for replica, block_replicas in scheduled_replicas:
        if block_replicas is None:
            replica.growing = False
            replica.group = Group.null_group
            # replica is a clone -> use inventory.update instead of inventory.register_update
            inventory.update(replica)

            original_replica = replica.site.find_dataset_replica(replica.dataset)
            for block_replica in original_replica.block_replicas:
                block_replica.group = Group.null_group
                inventory.register_update(block_replica)

        else:
            for block_replica in block_replicas:
                block_replica.group = Group.null_group
                inventory.update(block_replica)

# Update the inventory from non-reserved subscriptions

for (block, site), subs in done_subscriptions.iteritems():
    replica = block.find_replica(site)

    done_subscription_ids.extend(sub.id for sub in subs)

    if replica.file_ids is None:
        continue

    file_ids = set(replica.file_ids)
    file_ids.update(sub.file.id for sub in subs)

    all_file_ids = set()
    for f in block.files:
        if f.id == 0:
            all_file_ids.add(f.lfn)
        else:
            all_file_ids.add(f.id)

    if file_ids == all_file_ids:
        replica.size = block.size
        replica.file_ids = None
    else:
        replica.size += sum(sub.file.size for sub in subs)
        replica.file_ids = tuple(file_ids)

    inventory.register_update(replica)

for (block, site), subs in done_desubscriptions.iteritems():
    replica = block.find_replica(site)

    done_subscription_ids.extend(sub.id for sub in subs)

    if replica.file_ids is None:
        file_ids = set()
        for f in block.files:
            if f.id == 0:
                file_ids.add(f.lfn)
            else:
                file_ids.add(f.id)
    else:
        file_ids = set(replica.file_ids)

    file_ids -= set(sub.file.id for sub in subs)

    if len(file_ids) == 0:
        inventory.delete(replica)
    else:
        replica.size -= sum(sub.file.size for sub in subs)
        replica.file_ids = tuple(file_ids)
        inventory.register_update(replica)

# Make subscriptions for locally invalidated files

processed_ids = []
for inv_id, site_name, lfn in registry.db.query('SELECT `id`, `site`, `lfn` FROM `local_invalidations`'):
    processed_ids.append(inv_id)

    site = inventory.sites[site_name]
    lfile = inventory.find_file(lfn)

    if site is not None and lfile is not None:
        rlfsm.subscribe_file(site, lfile)

if authorized:
    registry.db.delete_many('local_invalidations', 'id', processed_ids)

# Make changes to the local databases
# This is dangerous though - if inventory update fails on the server side for some reason,
# we might not see the inconsistency for a while.

# Remove completed subscriptions
rlfsm.close_subscriptions(done_subscription_ids)

if authorized:
    # Remove completed reservations
    reservations_db.delete_many('phedex_transfer_reservations', 'id', done_transfer_reservation_ids)
    reservations_db.delete_many('phedex_deletion_reservations', 'id', done_deletion_reservation_ids)

    # Step 8 of PhEDEx updates
    if args.mode in ('ReplicaDelta', 'ReplicaFull') and os.path.exists(config.updater_state_file):
        state_db = sqlite3.connect(config.updater_state_file)
        cursor = state_db.cursor()
    
        # Additionally for replica updates
        if args.mode == 'ReplicaDelta':
            sql = 'INSERT INTO `replica_delta_updates` VALUES (?, ?, ?)'
            cursor.execute(sql, (update_start, len(updated_replicas), len(deleted_replicas)))
    
        elif args.mode == 'ReplicaFull':
            for site_name, dataset_name in site_dataset_combos:
                tier = dataset_name[dataset_name.rfind('/') + 1:]
                cursor.execute('DELETE FROM `replica_full_updates` WHERE `site` = ? AND `tier` = ?', (site_name, tier))
    
        state_db.commit()
        state_db.close()


LOG.info('Inventory update completed.')
